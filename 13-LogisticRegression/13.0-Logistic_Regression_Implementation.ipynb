{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8466666666666667\n",
      "Confusion Matrix:\n",
      " [[118  17]\n",
      " [ 29 136]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84       135\n",
      "           1       0.89      0.82      0.86       165\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.85      0.85      0.85       300\n",
      "weighted avg       0.85      0.85      0.85       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Step 1: Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To show plots in Jupyter notebooks\n",
    "%matplotlib inline\n",
    "# 👉 These are standard Python libraries used for:\n",
    "# •\tpandas, numpy → data handling\n",
    "# •\tmatplotlib, seaborn → data visualization\n",
    "\n",
    "# 🧱 Step 2: Create Synthetic Dataset for Binary Classification\n",
    "\n",
    "#make_classification: It's used to create fake (synthetic) data for training and testing\n",
    "#  machine learning models.\n",
    "from sklearn.datasets import make_classification \n",
    "\n",
    "# Create a dataset with 1000 samples, 10 features, and 2 classes (binary)\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=10, \n",
    "                           n_classes=2, \n",
    "                           random_state=42)\n",
    "#random_state=42: Makes sure you get the same random dataset every time you run the code\n",
    "# 👉 make_classification creates a fake dataset for classification.\n",
    "# •\tX = independent features (input)\n",
    "# •\ty = target (labels: 0 or 1)\n",
    "# •\trandom_state makes sure you get the same results every time\n",
    "\n",
    "#📊 Step 3: Convert to DataFrame (Optional) \n",
    "df = pd.DataFrame(X)\n",
    "df.head()\n",
    "\n",
    "# | Row | Feature\\_0 | Feature\\_1 | Feature\\_2 | Feature\\_3 | Feature\\_4 | Feature\\_5 | Feature\\_6 | Feature\\_7 | Feature\\_8 | Feature\\_9 |\n",
    "# | --- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n",
    "# | 0   | 0.9648     | -0.0664    | 0.9868     | -0.3581    | 0.9973     | 1.1819     | -1.6157    | -1.2102    | -0.6281    | 1.2273     |\n",
    "# | 1   | -0.9165    | -0.5664    | -1.0086    | 0.8316     | -1.1770    | 1.8205     | 1.7524     | -0.9845    | 0.3639     | 0.2095     |\n",
    "# | 2   | -0.1095    | -0.4328    | -0.4576    | 0.7938     | -0.2686    | -1.8364    | 1.2391     | -0.2464    | -1.0581    | -0.2974    |\n",
    "# | 3   | 1.7504     | 2.0236     | 1.6882     | 0.0068     | -1.6077    | 0.1847     | -2.6194    | -0.3574    | -1.4731    | -0.1900    |\n",
    "# | 4   | -0.2247    | -0.7113    | -0.2208    | 0.1171     | 1.5361     | 0.5975     | 0.3486     | -0.9392    | 0.1759     | 0.2362     |\n",
    "\n",
    "\n",
    "#✂️ Step 4: Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=42)\n",
    "\n",
    "#👉 Always split your data before training to evaluate performance on unseen data.\n",
    "\n",
    "# 🤖 Step 5: Train Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create the model \n",
    "model = LogisticRegression()\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 🔮 Step 6: Make Predictions\n",
    "# Predict the output for test data\n",
    "y_pred = model.predict(X_test)\n",
    "# 👉 y_pred will contain the predicted class (0 or 1) for the test set.\n",
    "\n",
    "#📈 Optional: Get Class Probabilities\n",
    "y_prob = model.predict_proba(X_test)\n",
    "y_prob[:5]\n",
    "# array([[0.77447791, 0.22552209],\n",
    "#        [0.0336685 , 0.9663315 ],\n",
    "#        [0.67068215, 0.32931785],\n",
    "#        [0.0798668 , 0.9201332 ],\n",
    "#        [0.97661665, 0.02338335]])\n",
    "\n",
    "# 🧠 Simple Summary\n",
    "# You're asking the model: \"How sure are you?\"\n",
    "# Instead of giving you just 0 or 1, it gives you something like:\n",
    "# ➤ [0.77, 0.22] → \"77% chance of class 0, 22% chance of class 1\"\n",
    "\n",
    "# 📏 Step 7: Evaluate the Model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Classification Report: shows precision, recall, f1-score\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# 👉 This step helps evaluate the model:\n",
    "# •\taccuracy_score → overall correct predictions\n",
    "# •\tconfusion_matrix → breakdown of TP, TN, FP, FN\n",
    "# •\tclassification_report → includes precision, recall, and F1-score\n",
    "\n",
    "\n",
    "# Accuracy: 0.8466666666666667\n",
    "# Confusion Matrix:\n",
    "#  [[118  17]\n",
    "#  [ 29 136]]\n",
    "# Classification Report:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.80      0.87      0.84       135\n",
    "#            1       0.89      0.82      0.86       165\n",
    "\n",
    "#     accuracy                           0.85       300\n",
    "#    macro avg       0.85      0.85      0.85       300\n",
    "# weighted avg       0.85      0.85      0.85       300\n",
    "\n",
    "# 118 (True Negatives): The model correctly predicted class 0 when it was actually class 0.\n",
    "\n",
    "# 17 (False Positives): The model predicted class 1, but it was actually class 0 (wrong prediction).\n",
    "\n",
    "# 29 (False Negatives): The model predicted class 0, but it was actually class 1 (wrong prediction).\n",
    "\n",
    "# 136 (True Positives): The model correctly predicted class 1 when it was actually class 1.\n",
    "\n",
    "\n",
    "# Actual=1, Predict =1 ->TruePositive(Good)\n",
    "# Actual=0, Predicted= 0->TrueNegative(Good)\n",
    "# Actual=0, Predicted= 1->FalsePositive(Blunder)\n",
    "# Actual=1, Predicted= 0->FalseNegative(fine)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
